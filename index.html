<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Domain agnostic Encoder for Fast Personalization of Text-to-Image Models">
  <meta name="keywords" content="Textual Inversion, Text-to-Image, Personalized Generation, Dreambooth, LORA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models</title>

<!--   Global site tag (gtag.js) - Google Analytics-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QVM9XP0Q0C"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-QVM9XP0Q0C');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Authors
            </span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> 

              <!-- Code Link. 
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
              -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="subtitle">
        <b>TL;DR </b>  We extend the domain-tuning encoder approach (E4T) to multiple-domains. </h2><br>
        <span class="tag is-medium is-rounded is-light is-success">
          <span class="icon">
            <i class="fas fa-paw"></i>
          </span>
          <span class="icon">
            <i class="fas fa-cat"></i>
          </span>
          <span> Multi-Domain</span>
        </span>
        <span class="tag is-medium is-rounded is-light is-info">
          <span class="icon"><i class="fas fa-fast-forward"></i></span>
          <span> Fast tuning</span>
        </span>
        <span class="tag is-medium is-rounded is-light is-warning">
          <span class="icon">
            <i class="fas fa-dice-one"></i>
          </span>
          <span> Single-shot</span>
        </span>
        
      </div>
    <br>
    <div class="hero-body">
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/figs/teaser.png" alt="Teaser."/>
	  </div>
    </div>
  </div>
 <!--  </div> -->
  </div>
  </div>
 <!--  </div> -->
</section>

<section class="section hero is-light">
  <div class="container is-max">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- div class="item">
          <p style="margin-bottom: 30px">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/video.mp4"
          type="video/mp4">
        </video>
        </p>
        </div -->
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
        <p>
        Text-to-image (T2I) personalization allows users to guide the creative image generation process by combining their own visual concepts in natural language prompts. Recently, encoder-based techniques have emerged as a new effective approach for T2I personalization, reducing the need for multiple images and long training times. However, most existing encoders are limited to a single-class domain, which hinders their ability to handle diverse concepts. In this work, we propose a domain-agnostic method that does not require any specialized dataset or prior information about the personalized concepts. We introduce a novel contrastive-based regularization technique to maintain high fidelity to the target concept characteristics while keeping the predicted embeddings close to editable regions of the latent space, by pushing the predicted tokens toward their nearest existing CLIP tokens. Our experimental results demonstrate the effectiveness of our approach and show how the learned tokens are more semantic than tokens predicted by unregularized models. This leads to a better representation that achieves state-of-the-art performance while being more flexible than previous methods.
        </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>



<section class="section">
  <div class="container is-max">
    <!-- Method. -->
    <div class="rows is-centered has-text-centered">
      <!-- Overview -->
      <div class="row is-max">
        <h2 class="title is-3">How does it work?</h2>
        <div class="row is-centered has-text-centered is-four-fifths">
          <img id="architecture" src="static/figs/overview_up.png" width=90%/>
        </div>
        <div class="columns is-centered has-text-centered">
          <div class="column content has-text-justified is-four-fifths">
            <p>
              We pre-train an encoder to predict <em>word-embeddings</em> and <em>Low-Rank weight updates.</em>  
              Our method consists of a feature-extraction backbone which follows the E4T approach and uses a mix of
              CLIP-features from the concept image, and denoiser-based features from the current noisy generation. 
              These features are fed into an embedding prediction head, and a hypernetwork which predicts LoRA-style attention-weight offsets.
              During inference, we predict LoRA weights and word-embedding and tune those on the target subject.
            </p>
          </div>
        </div>
      </div>
     <hr>
      <div class="row is-centered has-text-centered is-four-fifths">
          <div class="columns is-centered has-text-centered" >
            <!-- Overview - Left -->
            <div class="column content has-text-justified is-half">
              <div class="row is-centered has-text-centered"><img src="static/figs/overview_left.png" alt="Teaser." width=80%/></div>
              <div class="row is-centered has-text-centered">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-1"></div>
                  <div class="column content is-centered has-text-justified">
                      <p>
                        <b>Avoiding subject overfitting via dual-path adaptation: </b> We employ a dual-path adaptation approach where each attention branch is repeated twice, 
                        once using the soft-embedding and the hypernetwork offsets,
                        and once with the vanilla model and a hard-prompt containing the embedding's nearest neighbor. 
                        These branches are linearly blended to better preserve the prior. 
                        The latter path preserves the model's prior, while the new adapted branch adapts to the target concept.
                      </p>
                  </div>
                  <div class="column is-1"></div>
              </div>
              </div>
            </div>

            <!-- Overview - right -->
            <div class="column content has-text-justified is-half">
              <div class="row is-centered has-text-centered"><img src="static/figs/overview_right.png" alt="Teaser." width=80%/></div>
              <div class="row is-centered has-text-centered">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-1"></div>
                  <div class="column content is-centered has-text-justified">
                      <p>
                        <b>Push predictions towards CLIP word embeddings for better editability:</b>
                        Our embeddings are regularized using a nearest-neighbor-based contrastive loss that pushes them toward real words but away from the embeddings of other concepts. 
                        Intuitively, staying close to the real-word embedding manifold ensures the target embedding will blend well in novel prompts, preserving the prompt semantic.
                      </p>
                  </div>
                  <div class="column is-1"></div>
              </div>
              </div>
            </div>

          </div>
      </div>

    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-widescreen">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparisons to Prior Work</h2>
        <div class="content has-text-justified">
        </div>
        <div class="column is-centered has-text-centered is-widescreen">
        <img id="face_comps" src="static/figs/comparison.png"/>
      </div>
        <div class="content has-text-justified">
          <p>
            Qualitative comparison with existing methods. 
            Our method achieves comparable quality to the state-of-the-art using only a single image and 12 or fewer training steps. 
            Notably, it generalizes to unique objects which recent encoder-based methods struggle with.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-widescreen">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Additional Resutls</h2>
        <div class="content has-text-justified">
        </div>
        <div class="column is-centered has-text-centered is-widescreen">
        <img id="face_comps" src="static/figs/additional.png"/>
      </div>
        <div class="content has-text-justified">
          <p>
            Additional qualitative results generated using our method. 
            The left-most column shows the input image, followed by 4 personalized generations for each subject.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<!--<section class="section" id="BibTeX">-->
<!--  <div class="container is-max-desktop content">-->
<!--    <h2 class="title">BibTeX</h2>-->
<!--    <p>If you find our work useful, please cite our paper:</p>-->
<!--    <pre><code>Coming soon-->
<!--}</code></pre>-->
<!--  </div>-->
<!--</section>-->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>If you want an image removed from this page or have other requests, please contact us at <a href="datencoder@gmail.com">datencoder@gmail.com</a></p>
          <p>
            Website content is licensed under a <a rel="license"
                                                href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative
            Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
